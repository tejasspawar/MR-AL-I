{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ME781_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aA4nhts9jkKe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejasspawar/MR-AL-I/blob/main/ME781_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbnfPLq5lv2q"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "import math\n",
        "from torch.nn.init import kaiming_uniform_, kaiming_normal_\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
        "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyxAMuOyjeQD"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhi7VLkqjdyP"
      },
      "source": [
        "class MRDataset(data.Dataset):\n",
        "    def __init__(self, root_dir, task, plane, train=True, transform=None):\n",
        "        super().__init__()\n",
        "        self.task = task\n",
        "        self.plane = plane\n",
        "        self.root_dir = root_dir\n",
        "        self.train = train\n",
        "        if self.train:\n",
        "            self.folder_path = self.root_dir + 'train/{0}/'.format(plane)\n",
        "            self.records = pd.read_csv(\n",
        "                self.root_dir + 'train-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
        "        else:\n",
        "            transform = None\n",
        "            self.folder_path = self.root_dir + 'valid/{0}/'.format(plane)\n",
        "            self.records = pd.read_csv(\n",
        "                self.root_dir + 'valid-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
        "\n",
        "        self.records['id'] = self.records['id'].map(\n",
        "            lambda i: '0' * (4 - len(str(i))) + str(i))\n",
        "        self.paths = [self.folder_path + filename +\n",
        "                      '.npy' for filename in self.records['id'].tolist()]\n",
        "        self.labels = self.records['label'].tolist()\n",
        "\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        array = np.load(self.paths[index])\n",
        "        label = self.labels[index]\n",
        "        if label == 1:\n",
        "            label = torch.FloatTensor([[0, 1]])\n",
        "        elif label == 0:\n",
        "            label = torch.FloatTensor([[1, 0]])\n",
        "\n",
        "        if self.transform:\n",
        "            array = self.transform(array)\n",
        "\n",
        "        return array, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA4nhts9jkKe"
      },
      "source": [
        "# CNN Architechture\n",
        "\n",
        "Reference: https://arxiv.org/abs/2005.02706"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuaax0-feulm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e56ab71-c52f-45c4-9247-4008dd5fed01"
      },
      "source": [
        "%%file elnet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "from torch.nn.init import kaiming_uniform_, kaiming_normal_\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def make_deterministic(seed):\n",
        "\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def weight_init(m, seed=2, init_type='uniform'):\n",
        "\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "\n",
        "        if init_type == 'normal':\n",
        "            kaiming_normal_(m.weight)\n",
        "        else:\n",
        "            kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "    else:\n",
        "        raise TypeError(\"cannnot initialize such weights\")\n",
        "\n",
        "\n",
        "def get_norm_layer(channels, norm_type='layer'):\n",
        "    if norm_type == 'instance':\n",
        "        layer = nn.GroupNorm(channels, channels)\n",
        "    elif norm_type == 'batch':\n",
        "        layer = nn.BatchNorm2d(channels)\n",
        "    else:\n",
        "        layer = nn.GroupNorm(1, channels)  # layer norm by default\n",
        "\n",
        "    return layer\n",
        "\n",
        "\n",
        "def conv_block(channels, kernel_size, dilation=1, repeats=2, normalization='layer', seed=2, init_type='uniform'):\n",
        "    \"\"\"\n",
        "    :param channels: the input channel amount (same for output)\n",
        "    :param kernel_size: 2D convolution kernel\n",
        "    :param dilation: the dialation for the kernels of a conv block\n",
        "    :param padding: amount of padding\n",
        "    :param repeats: amount of repeats before added with identity\n",
        "    :param normalization: the type of multi-slice normalization used\n",
        "    :param seed: which seed of initial weights to use\n",
        "    :param init_type: which type of Kaiming Init to use\n",
        "    :return: nn.Sequential(for the given block)\n",
        "    \"\"\"\n",
        "\n",
        "    conv_list = nn.ModuleList([])\n",
        "\n",
        "    for i in range(repeats):\n",
        "        conv2d = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,\n",
        "                           dilation=dilation, stride=1, bias=False,\n",
        "                           padding=(kernel_size + ((dilation - 1) * (kernel_size - 1))) // 2)\n",
        "        weight_init(conv2d, seed=seed, init_type=init_type)\n",
        "        conv_list.append(conv2d)\n",
        "\n",
        "        #   Instance Normalization and Layer Normalization are just variations of Group Normalization\n",
        "        #   https://pytorch.org/docs/stable/nn.html#groupnorm\n",
        "\n",
        "        conv_list.append(get_norm_layer(channels, normalization))\n",
        "        conv_list.append(nn.ReLU())\n",
        "\n",
        "    return nn.Sequential(*conv_list)\n",
        "\n",
        "\n",
        "def get_antialiasing_filter(kernel_size):\n",
        "    \"\"\"Get an integer specifying the 2D kernel size >>> returns a (1 x 1 x kernel_size x kernel_size)\"\"\"\n",
        "\n",
        "    kernel_dict = {\n",
        "        1: [[[[1.]]]],\n",
        "\n",
        "        2: [[[[0.2500, 0.2500],\n",
        "              [0.2500, 0.2500]]]],\n",
        "\n",
        "        3: [[[[0.0625, 0.1250, 0.0625],\n",
        "              [0.1250, 0.2500, 0.1250],\n",
        "              [0.0625, 0.1250, 0.0625]]]],\n",
        "\n",
        "        4: [[[[0.0156, 0.0469, 0.0469, 0.0156],\n",
        "              [0.0469, 0.1406, 0.1406, 0.0469],\n",
        "              [0.0469, 0.1406, 0.1406, 0.0469],\n",
        "              [0.0156, 0.0469, 0.0469, 0.0156]]]],\n",
        "\n",
        "        5: [[[[0.0039, 0.0156, 0.0234, 0.0156, 0.0039],\n",
        "              [0.0156, 0.0625, 0.0938, 0.0625, 0.0156],\n",
        "              [0.0234, 0.0938, 0.1406, 0.0938, 0.0234],\n",
        "              [0.0156, 0.0625, 0.0938, 0.0625, 0.0156],\n",
        "              [0.0039, 0.0156, 0.0234, 0.0156, 0.0039]]]],\n",
        "\n",
        "        6: [[[[0.0010, 0.0049, 0.0098, 0.0098, 0.0049, 0.0010],\n",
        "              [0.0049, 0.0244, 0.0488, 0.0488, 0.0244, 0.0049],\n",
        "              [0.0098, 0.0488, 0.0977, 0.0977, 0.0488, 0.0098],\n",
        "              [0.0098, 0.0488, 0.0977, 0.0977, 0.0488, 0.0098],\n",
        "              [0.0049, 0.0244, 0.0488, 0.0488, 0.0244, 0.0049],\n",
        "              [0.0010, 0.0049, 0.0098, 0.0098, 0.0049, 0.0010]]]],\n",
        "\n",
        "        7: [[[[0.0002, 0.0015, 0.0037, 0.0049, 0.0037, 0.0015, 0.0002],\n",
        "              [0.0015, 0.0088, 0.0220, 0.0293, 0.0220, 0.0088, 0.0015],\n",
        "              [0.0037, 0.0220, 0.0549, 0.0732, 0.0549, 0.0220, 0.0037],\n",
        "              [0.0049, 0.0293, 0.0732, 0.0977, 0.0732, 0.0293, 0.0049],\n",
        "              [0.0037, 0.0220, 0.0549, 0.0732, 0.0549, 0.0220, 0.0037],\n",
        "              [0.0015, 0.0088, 0.0220, 0.0293, 0.0220, 0.0088, 0.0015],\n",
        "              [0.0002, 0.0015, 0.0037, 0.0049, 0.0037, 0.0015, 0.0002]]]]\n",
        "\n",
        "    }\n",
        "\n",
        "    if kernel_size in kernel_dict:\n",
        "        return torch.Tensor(kernel_dict[kernel_size])\n",
        "    else:\n",
        "        raise ValueError('Unrecognized kernel size')\n",
        "\n",
        "\n",
        "class BlurPool(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, stride, filter_size=3):\n",
        "        super(BlurPool, self).__init__()\n",
        "        self.channels = channels  # same input and output channels\n",
        "        self.filter_size = filter_size\n",
        "        self.stride = stride\n",
        "        '''Kernel is a 1x5x5 kernel'''\n",
        "\n",
        "        # repeat tensor from (1 x 1 x fs x fs) >>> (channels x 1 x fs x fs)\n",
        "        self.kernel = nn.Parameter(get_antialiasing_filter(filter_size).repeat(self.channels, 1, 1, 1),\n",
        "                                   requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x is a tensor of dimension (batch, in_channels, height, width)\n",
        "        - assume same input and output channels, and groups = 1\n",
        "        - CURRENTLY DON'T SUPPORT PADDING\n",
        "        \"\"\"\n",
        "\n",
        "        y = F.conv2d(input=x, weight=self.kernel, stride=self.stride, groups=self.channels)\n",
        "        return y\n",
        "\n",
        "    def to(self, dtype):\n",
        "        self.kernel = self.kernel.to(dtype)\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "class ELNet(nn.Module):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.K = kwargs.get('K', 4)  # default K for ELNet\n",
        "        self.norm_type = kwargs.get('norm_type', 'layer')  # default multi-slice normalization\n",
        "        self.aa_filter = kwargs.get('aa_filter_size', 5)  # default aa-filter configuration\n",
        "        self.weight_init_type = kwargs.get('weight_init_type', 'normal')  # type of weight initialization\n",
        "        self.seed = kwargs.get('seed', 2)  # default seed for initialization\n",
        "        self.num_classes = kwargs.get('num_classes', 2)  # number of classes for ELNet\n",
        "\n",
        "        make_deterministic(self.seed)\n",
        "\n",
        "        if isinstance(self.aa_filter, int):\n",
        "            aa_filter_size = [self.aa_filter] * 5\n",
        "            print(aa_filter_size)\n",
        "\n",
        "        self.channel_config = [4 * self.K, 8 * self.K, 16 * self.K, 16 * self.K, 16 * self.K]\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(1, self.channel_config[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.downsample_1 = BlurPool(channels=self.channel_config[0], stride=2, filter_size=aa_filter_size[0])\n",
        "        self.norm_1 = get_norm_layer(self.channel_config[0], norm_type=self.norm_type)\n",
        "\n",
        "        self.conv_2 = conv_block(self.channel_config[0], kernel_size=5, repeats=2, normalization=self.norm_type)\n",
        "        self.conv_2_to_3 = nn.Conv2d(self.channel_config[0], self.channel_config[1], kernel_size=5, stride=1, padding=2,\n",
        "                                     bias=False)\n",
        "        self.downsample_2 = BlurPool(channels=self.channel_config[1], stride=2, filter_size=aa_filter_size[1])\n",
        "\n",
        "        self.conv_3 = conv_block(self.channel_config[1], kernel_size=3, repeats=2, normalization=self.norm_type)\n",
        "        self.conv_3_to_4 = nn.Conv2d(self.channel_config[1], self.channel_config[2], kernel_size=3, stride=1, padding=1,\n",
        "                                     bias=False)\n",
        "        self.downsample_3 = BlurPool(channels=self.channel_config[2], stride=2, filter_size=aa_filter_size[2])\n",
        "\n",
        "        self.conv_4 = conv_block(self.channel_config[2], kernel_size=3, repeats=1, normalization=self.norm_type)\n",
        "        self.conv_4_to_5 = nn.Conv2d(self.channel_config[2], self.channel_config[3], kernel_size=3, stride=1, padding=1,\n",
        "                                     bias=False)\n",
        "        self.downsample_4 = BlurPool(channels=self.channel_config[3], stride=2, filter_size=aa_filter_size[3])\n",
        "\n",
        "        self.conv_5 = conv_block(self.channel_config[3], kernel_size=3, repeats=1, normalization=self.norm_type)\n",
        "        self.conv_5_to_6 = nn.Conv2d(self.channel_config[3], self.channel_config[4], kernel_size=3, stride=1, padding=1,\n",
        "                                     bias=False)\n",
        "        self.downsample_5 = BlurPool(channels=self.channel_config[4], stride=2, filter_size=aa_filter_size[4])\n",
        "\n",
        "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        self.feature_dp = nn.Dropout(0.7)\n",
        "\n",
        "        self.fc = nn.Linear(self.channel_config[4], self.num_classes)\n",
        "\n",
        "        weight_init(self.conv_1, self.seed, self.weight_init_type)\n",
        "        weight_init(self.conv_2_to_3, self.seed, self.weight_init_type)\n",
        "        weight_init(self.conv_3_to_4, self.seed, self.weight_init_type)\n",
        "        weight_init(self.conv_4_to_5, self.seed, self.weight_init_type)\n",
        "        weight_init(self.conv_5_to_6, self.seed, self.weight_init_type)\n",
        "        weight_init(self.fc, self.seed, self.weight_init_type)\n",
        "\n",
        "    def feature_extraction(self, x):\n",
        "        x = x.permute(1,0, 2, 3)\n",
        "        #print(x.size())\n",
        "\n",
        "        if self.training:\n",
        "\n",
        "            x = self.downsample_1(F.relu(self.norm_1(self.conv_1(x))))\n",
        "\n",
        "            x = x + self.conv_2(x)  # skip connection (survival rate 1 for first skip)\n",
        "            x = self.downsample_2(F.relu(self.conv_2_to_3(x)))\n",
        "\n",
        "            x = x + self.conv_3(x)\n",
        "            x = self.downsample_3(F.relu(self.conv_3_to_4(x)))\n",
        "\n",
        "            x = x + self.conv_4(x)\n",
        "            x = self.downsample_4(F.relu(self.conv_4_to_5(x)))\n",
        "\n",
        "            x = x + self.conv_5(x)\n",
        "            x = self.downsample_5(F.relu(self.conv_5_to_6(x)))\n",
        "\n",
        "        else:  # evaluation mode\n",
        "\n",
        "            x = self.downsample_1(F.relu(self.norm_1(self.conv_1(x)), inplace=True))\n",
        "\n",
        "            x += self.conv_2(x)  # in-place skip connection (not suitable for training pass)\n",
        "            x = self.downsample_2(F.relu(self.conv_2_to_3(x), inplace=True))\n",
        "\n",
        "            x += self.conv_3(x)\n",
        "            x = self.downsample_3(F.relu(self.conv_3_to_4(x), inplace=True))\n",
        "\n",
        "            x += self.conv_4(x)  # skip connection\n",
        "            x = self.downsample_4(F.relu(self.conv_4_to_5(x), inplace=True))\n",
        "\n",
        "            x += self.conv_5(x)  # skip connection\n",
        "            x = self.downsample_5(F.relu(self.conv_5_to_6(x), inplace=True))\n",
        "\n",
        "        feats = nn.AdaptiveMaxPool2d(1)(x)  # get [sx16Kx1x1]\n",
        "        feats = self.feature_dp(feats)  # performs feature-wise dropout\n",
        "\n",
        "        return feats\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.feature_extraction(x)  # get [sx16Kx1x1]\n",
        "        feats = feats.squeeze(3)\n",
        "        feats = feats.permute(2, 1, 0)  # [1x16Kxs]\n",
        "\n",
        "        # classifier\n",
        "        feats = self.max_pool(feats).squeeze(2)  # [1x16K]\n",
        "        scores = self.fc(feats)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting elnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8CYrMkZj8wU"
      },
      "source": [
        "#Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhn7EQOcpU1w",
        "outputId": "e29c6791-b1d8-4e1c-8f39-46a96f65aa06"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CiJJTs0pYoe",
        "outputId": "384b301b-b962-42cd-8596-35ecfd736695"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elnet.py  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lsVZia-o9Mn"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from elnet import *\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5N_UZbwjxlr"
      },
      "source": [
        "def check(input_img):\n",
        "    \n",
        "    print(\" your image is : \" + input_img)\n",
        "   \n",
        "    input_img1=f'../app/images/{input_img}'\n",
        "    img = np.load(input_img1)\n",
        "    img=img/255\n",
        "    img=torch.Tensor(img)\n",
        "    img=img.repeat(1,1,1,1)\n",
        "    print(img.size())\n",
        "    l=['abnormal','meniscus']\n",
        "    model_path=[]\n",
        "    elNet=[]\n",
        "    output=[]\n",
        "    probas=[]\n",
        "    label={}\n",
        "    model_name = ['model_patience_5 _gamma_2n_abnormal_axial_val_auc_0.8636_train_auc_0.9016_epoch_39.pth','model_patience_5 _gamma_2n_meniscus_coronal_val_auc_0.7868_train_auc_0.7720_epoch_32.pth']     \n",
        "    for i in range(0,2):\n",
        "        model_name1=model_name[i]\n",
        "        m = f'../app/models/{model_name1}'\n",
        "        model_path.append(m)\n",
        "        elNet.append(torch.load(model_path[i],map_location='cpu'))\n",
        "        _ = elNet[i].eval()\n",
        "        x=elNet[i].forward(img.float())\n",
        "        output.append(x)\n",
        "        y=torch.sigmoid(x)\n",
        "        probas.append(y)\n",
        "        label[l[i]]=(probas[i][0][1].item())\n",
        "    print(label)        \n",
        "    return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBkr3vlxm6Si"
      },
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftoVO-Zk-l8Z"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2hAVSUmcfno",
        "outputId": "a2d59dc8-5699-4854-d201-3a2b5558c459"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  elnet,py  elnet.py  __pycache__\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdo7MOBqc5oG",
        "outputId": "7e74c25d-acca-4d92-91fd-f0ce5942a2a3"
      },
      "source": [
        "cd drive/MyDrive/Knee_MRI"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Knee_MRI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp5Nvl3ontxF",
        "outputId": "4fdb1a67-1cc3-45f4-fd77-bfd981c2d871"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta22vq3lQO5b"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "# from torchsample.transforms import RandomRotate, RandomTranslate, RandomFlip, ToTensor, Compose, RandomAffine\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from eldata import MRDataset\n",
        "import elnet\n",
        "\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUZLkW_1rmvz"
      },
      "source": [
        "# Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Knee_MRI')\n",
        "\n",
        "# Import your module or file\n",
        "import elnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEjkok-rCqCt"
      },
      "source": [
        "def train_model(elnet, train_loader, epoch, num_epochs, optimizer, writer, current_lr, log_every=100):\n",
        "    _ = elnet.train()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        elnet.cuda()\n",
        "\n",
        "    y_preds = []\n",
        "    y_trues = []\n",
        "    losses = []\n",
        "\n",
        "    for i, (image, label) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            # weight = weight.cuda()\n",
        "\n",
        "        label = label[0]\n",
        "        #print(label)\n",
        "        #weight = weight[0]\n",
        "        #output=elnet(image)\n",
        "\n",
        "        prediction = elnet.forward(image.float())\n",
        "\n",
        "        loss = torch.nn.BCEWithLogitsLoss()(prediction, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_value = loss.item()\n",
        "        losses.append(loss_value)\n",
        "\n",
        "        probas = torch.sigmoid(prediction)\n",
        "\n",
        "        y_trues.append(int(label[0][1]))\n",
        "        y_preds.append(probas[0][1].item())\n",
        "\n",
        "        try:\n",
        "            auc = metrics.roc_auc_score(y_trues, y_preds)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "\n",
        "        writer.add_scalar('Train/Loss', loss_value,\n",
        "                          epoch * len(train_loader) + i)\n",
        "        writer.add_scalar('Train/AUC', auc, epoch * len(train_loader) + i)\n",
        "\n",
        "        if (i % log_every == 0) & (i > 0):\n",
        "            print('''[Epoch: {0} / {1} |Single batch number : {2} / {3} ]| avg train loss {4} | train auc : {5} | lr : {6}'''.\n",
        "                  format(\n",
        "                      epoch + 1,\n",
        "                      num_epochs,\n",
        "                      i,\n",
        "                      len(train_loader),\n",
        "                      np.round(np.mean(losses), 4),\n",
        "                      np.round(auc, 4),\n",
        "                      current_lr\n",
        "                  )\n",
        "                  )\n",
        "\n",
        "    writer.add_scalar('Train/AUC_epoch', auc, epoch + i)\n",
        "\n",
        "    train_loss_epoch = np.round(np.mean(losses), 4)\n",
        "    train_auc_epoch = np.round(auc, 4)\n",
        "    return train_loss_epoch, train_auc_epoch\n",
        "\n",
        "\n",
        "def evaluate_model(elnet, val_loader, epoch, num_epochs, writer, current_lr, log_every=20):\n",
        "    _ = elnet.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        elnet.cuda()\n",
        "\n",
        "    y_trues = []\n",
        "    y_preds = []\n",
        "    losses = []\n",
        "\n",
        "    for i, (image, label) in enumerate(val_loader):\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            #weight = weight.cuda()\n",
        "\n",
        "        label = label[0]\n",
        "       # weight = weight[0]\n",
        "\n",
        "        prediction = elnet.forward(image.float())\n",
        "\n",
        "        loss = torch.nn.BCEWithLogitsLoss( )(prediction, label)\n",
        "\n",
        "        loss_value = loss.item()\n",
        "        losses.append(loss_value)\n",
        "\n",
        "        probas = torch.sigmoid(prediction)\n",
        "\n",
        "        y_trues.append(int(label[0][1]))\n",
        "        y_preds.append(probas[0][1].item())\n",
        "\n",
        "        try:\n",
        "            auc = metrics.roc_auc_score(y_trues, y_preds)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "\n",
        "        writer.add_scalar('Val/Loss', loss_value, epoch * len(val_loader) + i)\n",
        "        writer.add_scalar('Val/AUC', auc, epoch * len(val_loader) + i)\n",
        "\n",
        "        if (i % log_every == 0) & (i > 0):\n",
        "            print('''[Epoch: {0} / {1} |Single batch number : {2} / {3} ] | avg val loss {4} | val auc : {5} | lr : {6}'''.\n",
        "                  format(\n",
        "                      epoch + 1,\n",
        "                      num_epochs, \n",
        "                      i,\n",
        "                      len(val_loader),\n",
        "                      np.round(np.mean(losses), 4),\n",
        "                      np.round(auc, 4),\n",
        "                      current_lr\n",
        "                  )\n",
        "                  )\n",
        "\n",
        "    writer.add_scalar('Val/AUC_epoch', auc, epoch + i)\n",
        "\n",
        "    val_loss_epoch = np.round(np.mean(losses), 4)\n",
        "    val_auc_epoch = np.round(auc, 4)\n",
        "    return val_loss_epoch, val_auc_epoch\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLDw1Gq9osoV"
      },
      "source": [
        "def run(task, plane, prefix_name):\n",
        "  lr= 3e-5\n",
        "  lr_scheduler='plateau'\n",
        "  augment=1\n",
        "  gamma = 4\n",
        "  epochs= 50\n",
        "  flush_history=0\n",
        "  save_model = 1\n",
        "  patience = 10\n",
        "  log_every=100\n",
        "  log_root_folder = \"./logs/{0}/{1}/\".format(task, plane)\n",
        "  if flush_history == 1:\n",
        "          objects = os.listdir(log_root_folder)\n",
        "          for f in objects:\n",
        "              if os.path.isdir(log_root_folder + f):\n",
        "                  shutil.rmtree(log_root_folder + f)\n",
        "\n",
        "  now = datetime.now()\n",
        "  logdir = log_root_folder + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
        "  os.makedirs(logdir)\n",
        "  writer = SummaryWriter(logdir)\n",
        "  '''\n",
        "  def fnc(x):\n",
        "      return x.repeat(1,1,1,1).permute(0,1,2,3)\n",
        "  \n",
        "  def fnc1(x):\n",
        "      return torch.Tensor(x)\n",
        "'''\n",
        "  augmentor = transforms.Compose([\n",
        "      transforms.Lambda(lambda x: torch.Tensor(x)),\n",
        "      transforms.RandomRotation(degrees=(-10, 10)),\n",
        "      #transforms.RandomTranslate([0.11, 0.11]),\n",
        "      transforms.RandomHorizontalFlip(0.5),\n",
        "      #transforms.RandomFlip(),\n",
        "      transforms.RandomAffine(degrees=(10, 20), translate=(0.1, 0.1), scale=(0.8,0.8)),\n",
        "    \n",
        "      #transforms.Lambda(lambda x: x.repeat(1, 1, 1, 1).permute(0, 1, 2, 3)),\n",
        "      ])\n",
        "\n",
        "  train_dataset = MRDataset('./data/', task,plane, transform=augmentor, train=True)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
        "\n",
        "  validation_dataset = MRDataset( './data/', task, plane, train=False)\n",
        "  validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=-True, num_workers=0, drop_last=False)\n",
        "\n",
        "  Elnet = elnet.ELNet()\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      \n",
        "      Elnet = Elnet.cuda()\n",
        "\n",
        "  optimizer = optim.Adam(Elnet.parameters(), lr=lr)\n",
        "\n",
        "  if lr_scheduler == \"plateau\":\n",
        "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "          optimizer, factor=.3, threshold=1e-4, verbose=True)\n",
        "  elif lr_scheduler == \"step\":\n",
        "      scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "          optimizer, step_size=3, gamma=gamma)\n",
        "\n",
        "  best_val_loss = float('inf')\n",
        "  best_val_auc = float(0)\n",
        "\n",
        "  num_epochs = epochs\n",
        "  iteration_change_loss = 0\n",
        "  patience = patience\n",
        "  log_every = log_every\n",
        "\n",
        "  t_start_training = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      current_lr = get_lr(optimizer)\n",
        "\n",
        "      t_start = time.time()\n",
        "      \n",
        "      train_loss, train_auc = train_model(Elnet, train_loader, epoch, num_epochs, optimizer, writer, current_lr, log_every)\n",
        "      val_loss, val_auc = evaluate_model(Elnet, validation_loader, epoch, num_epochs, writer, current_lr)\n",
        "\n",
        "      if lr_scheduler == 'plateau':\n",
        "          scheduler.step(val_loss)\n",
        "      elif lr_scheduler == 'step':\n",
        "          scheduler.step()\n",
        "\n",
        "      t_end = time.time()\n",
        "      delta = t_end - t_start\n",
        "\n",
        "      print(\"train loss : {0} | train auc {1} | val loss {2} | val auc {3} | elapsed time {4} s\".format(train_loss, train_auc, val_loss, val_auc, delta))\n",
        "\n",
        "      iteration_change_loss += 1\n",
        "      print('-' * 30)\n",
        "\n",
        "      if val_auc > best_val_auc:\n",
        "          best_val_auc = val_auc\n",
        "          if bool(save_model):\n",
        "              file_name = f'model_{prefix_name}_{task}_{plane}_val_auc_{val_auc:0.4f}_train_auc_{train_auc:0.4f}_epoch_{epoch+1}.pth'\n",
        "              for f in os.listdir('./data/models/'):\n",
        "                  if (task in f) and (plane in f) and (prefix_name in f):\n",
        "                      os.remove(f'./data/models/{f}')\n",
        "              torch.save(Elnet, f'./data/models/{file_name}')\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          iteration_change_loss = 0\n",
        "\n",
        "      if iteration_change_loss == patience:\n",
        "          print('Early stopping after {0} iterations without the decrease of the val loss'.\n",
        "                format(iteration_change_loss))\n",
        "          break\n",
        "  # return best_loss\n",
        "  t_end_training = time.time()\n",
        "  print(f'training took {t_end_training - t_start_training} s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhR7RI9gowEy",
        "outputId": "06a99950-9f4c-456e-f63e-a508c20bf7a9"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  run('acl','axial', 'ACL_Axial')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.625 | train auc : 0.5667 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.5646 | train auc : 0.5547 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.5441 | train auc : 0.5515 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.5297 | train auc : 0.5671 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.5151 | train auc : 0.5678 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.4999 | train auc : 0.5882 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.5079 | train auc : 0.5967 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.5085 | train auc : 0.5981 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.5091 | train auc : 0.5985 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.4927 | train auc : 0.6113 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.4849 | train auc : 0.6288 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.6205 | val auc : 0.8818 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.6428 | val auc : 0.8048 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.6335 | val auc : 0.789 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.6377 | val auc : 0.7789 | lr : 3e-05\n",
            "[Epoch: 1 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.638 | val auc : 0.7639 | lr : 3e-05\n",
            "train loss : 0.4839 | train auc 0.6265 | val loss 0.6434 | val auc 0.7478 | elapsed time 819.1581604480743 s\n",
            "------------------------------\n",
            "[Epoch: 2 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.4083 | train auc : 0.686 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.4087 | train auc : 0.699 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3973 | train auc : 0.713 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.4239 | train auc : 0.7136 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.4301 | train auc : 0.7232 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.4481 | train auc : 0.7059 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.4523 | train auc : 0.7026 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.4439 | train auc : 0.7059 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.452 | train auc : 0.6916 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.4523 | train auc : 0.7012 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.4462 | train auc : 0.7019 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5678 | val auc : 0.8654 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.625 | val auc : 0.8667 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.6081 | val auc : 0.8463 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.6126 | val auc : 0.809 | lr : 3e-05\n",
            "[Epoch: 2 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.6323 | val auc : 0.773 | lr : 3e-05\n",
            "train loss : 0.4451 | train auc 0.7033 | val loss 0.6265 | val auc 0.775 | elapsed time 190.97815942764282 s\n",
            "------------------------------\n",
            "[Epoch: 3 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.4057 | train auc : 0.6667 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.4297 | train auc : 0.7309 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.4308 | train auc : 0.756 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.4514 | train auc : 0.7335 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.4362 | train auc : 0.7388 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.419 | train auc : 0.7475 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.4234 | train auc : 0.7502 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.4176 | train auc : 0.7509 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.4224 | train auc : 0.7451 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.4304 | train auc : 0.7374 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.4279 | train auc : 0.7334 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5755 | val auc : 0.8558 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5533 | val auc : 0.9225 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5728 | val auc : 0.8604 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5783 | val auc : 0.834 | lr : 3e-05\n",
            "[Epoch: 3 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5804 | val auc : 0.8423 | lr : 3e-05\n",
            "train loss : 0.4295 | train auc 0.7372 | val loss 0.585 | val auc 0.8325 | elapsed time 185.94603323936462 s\n",
            "------------------------------\n",
            "[Epoch: 4 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3853 | train auc : 0.7478 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3851 | train auc : 0.7895 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3838 | train auc : 0.7789 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.4116 | train auc : 0.7627 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.4026 | train auc : 0.7704 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.4131 | train auc : 0.7453 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.4232 | train auc : 0.7504 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.4232 | train auc : 0.7459 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.4234 | train auc : 0.7492 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.4213 | train auc : 0.7498 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.4229 | train auc : 0.7483 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5771 | val auc : 0.8942 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5984 | val auc : 0.85 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5705 | val auc : 0.8719 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.6627 | val auc : 0.8391 | lr : 3e-05\n",
            "[Epoch: 4 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.6943 | val auc : 0.8235 | lr : 3e-05\n",
            "train loss : 0.4174 | train auc 0.751 | val loss 0.6731 | val auc 0.8109 | elapsed time 181.24592638015747 s\n",
            "------------------------------\n",
            "[Epoch: 5 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3873 | train auc : 0.792 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3982 | train auc : 0.7539 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3618 | train auc : 0.7883 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3629 | train auc : 0.7614 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3828 | train auc : 0.7531 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.388 | train auc : 0.754 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3866 | train auc : 0.7537 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3912 | train auc : 0.7527 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3998 | train auc : 0.7647 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.4017 | train auc : 0.764 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.4107 | train auc : 0.7524 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5183 | val auc : 0.9286 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5498 | val auc : 0.9175 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5733 | val auc : 0.8446 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5934 | val auc : 0.8173 | lr : 3e-05\n",
            "[Epoch: 5 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5924 | val auc : 0.8308 | lr : 3e-05\n",
            "train loss : 0.4134 | train auc 0.747 | val loss 0.5992 | val auc 0.8507 | elapsed time 188.49563694000244 s\n",
            "------------------------------\n",
            "[Epoch: 6 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3563 | train auc : 0.8327 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3974 | train auc : 0.7909 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.401 | train auc : 0.7783 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3908 | train auc : 0.8035 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3694 | train auc : 0.821 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3739 | train auc : 0.806 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3903 | train auc : 0.7802 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.385 | train auc : 0.7844 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.4011 | train auc : 0.7741 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.4056 | train auc : 0.7697 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.4066 | train auc : 0.7629 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5332 | val auc : 0.8673 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5328 | val auc : 0.8922 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.543 | val auc : 0.8756 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5594 | val auc : 0.8809 | lr : 3e-05\n",
            "[Epoch: 6 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5593 | val auc : 0.881 | lr : 3e-05\n",
            "train loss : 0.4062 | train auc 0.7633 | val loss 0.5771 | val auc 0.8423 | elapsed time 189.32843375205994 s\n",
            "------------------------------\n",
            "[Epoch: 7 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.4164 | train auc : 0.8489 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3674 | train auc : 0.8439 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3878 | train auc : 0.824 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.4003 | train auc : 0.7967 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3907 | train auc : 0.7951 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3885 | train auc : 0.7905 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3891 | train auc : 0.7839 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3928 | train auc : 0.7749 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3913 | train auc : 0.78 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3902 | train auc : 0.7766 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3895 | train auc : 0.7796 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5563 | val auc : 0.9273 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5758 | val auc : 0.8929 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5439 | val auc : 0.8911 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5781 | val auc : 0.8689 | lr : 3e-05\n",
            "[Epoch: 7 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5806 | val auc : 0.8537 | lr : 3e-05\n",
            "train loss : 0.3864 | train auc 0.7852 | val loss 0.5639 | val auc 0.862 | elapsed time 184.84442377090454 s\n",
            "------------------------------\n",
            "[Epoch: 8 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3947 | train auc : 0.693 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.404 | train auc : 0.7269 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3993 | train auc : 0.7446 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3866 | train auc : 0.7621 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3874 | train auc : 0.7674 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3867 | train auc : 0.7834 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3939 | train auc : 0.7802 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3953 | train auc : 0.7667 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3906 | train auc : 0.7694 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3961 | train auc : 0.7714 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3995 | train auc : 0.7698 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.4757 | val auc : 0.9327 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5367 | val auc : 0.8231 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5776 | val auc : 0.7883 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5784 | val auc : 0.8323 | lr : 3e-05\n",
            "[Epoch: 8 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5865 | val auc : 0.8397 | lr : 3e-05\n",
            "train loss : 0.4 | train auc 0.7692 | val loss 0.589 | val auc 0.849 | elapsed time 184.86877489089966 s\n",
            "------------------------------\n",
            "[Epoch: 9 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.4088 | train auc : 0.7738 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.4163 | train auc : 0.752 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.4412 | train auc : 0.74 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.4473 | train auc : 0.7554 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.4353 | train auc : 0.7618 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.4154 | train auc : 0.7814 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.4194 | train auc : 0.7713 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.4115 | train auc : 0.7756 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.4038 | train auc : 0.7856 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3943 | train auc : 0.7845 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3855 | train auc : 0.7877 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5325 | val auc : 0.8426 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5605 | val auc : 0.8043 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5589 | val auc : 0.8344 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5618 | val auc : 0.8346 | lr : 3e-05\n",
            "[Epoch: 9 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5542 | val auc : 0.8569 | lr : 3e-05\n",
            "train loss : 0.3886 | train auc 0.7833 | val loss 0.5683 | val auc 0.8451 | elapsed time 184.66435170173645 s\n",
            "------------------------------\n",
            "[Epoch: 10 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3607 | train auc : 0.7945 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3318 | train auc : 0.8617 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3393 | train auc : 0.8295 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3256 | train auc : 0.8559 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3267 | train auc : 0.8543 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3426 | train auc : 0.8198 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3562 | train auc : 0.805 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3652 | train auc : 0.8018 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3835 | train auc : 0.7951 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3777 | train auc : 0.7962 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3763 | train auc : 0.7984 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.4607 | val auc : 0.8778 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.6047 | val auc : 0.9071 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5972 | val auc : 0.8849 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5719 | val auc : 0.8704 | lr : 3e-05\n",
            "[Epoch: 10 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5965 | val auc : 0.8443 | lr : 3e-05\n",
            "train loss : 0.381 | train auc 0.7922 | val loss 0.5728 | val auc 0.8763 | elapsed time 185.46585893630981 s\n",
            "------------------------------\n",
            "[Epoch: 11 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.4324 | train auc : 0.7524 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.4426 | train auc : 0.7113 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3982 | train auc : 0.7571 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.377 | train auc : 0.7916 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3727 | train auc : 0.7977 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.395 | train auc : 0.7853 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3885 | train auc : 0.7769 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3838 | train auc : 0.789 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3724 | train auc : 0.7961 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.377 | train auc : 0.7998 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3766 | train auc : 0.7982 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5706 | val auc : 0.7692 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5651 | val auc : 0.848 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5204 | val auc : 0.8902 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.59 | val auc : 0.8415 | lr : 3e-05\n",
            "[Epoch: 11 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5871 | val auc : 0.8726 | lr : 3e-05\n",
            "train loss : 0.3738 | train auc 0.8022 | val loss 0.5804 | val auc 0.8634 | elapsed time 185.3487319946289 s\n",
            "------------------------------\n",
            "[Epoch: 12 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.4326 | train auc : 0.7157 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.4286 | train auc : 0.7377 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3808 | train auc : 0.757 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3773 | train auc : 0.7578 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3729 | train auc : 0.7841 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3746 | train auc : 0.7888 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3759 | train auc : 0.7884 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.375 | train auc : 0.7921 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3766 | train auc : 0.7904 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3733 | train auc : 0.7937 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3696 | train auc : 0.8021 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5538 | val auc : 0.9038 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5346 | val auc : 0.8762 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5055 | val auc : 0.9011 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5365 | val auc : 0.8634 | lr : 3e-05\n",
            "[Epoch: 12 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5404 | val auc : 0.854 | lr : 3e-05\n",
            "train loss : 0.37 | train auc 0.8056 | val loss 0.535 | val auc 0.8625 | elapsed time 181.47968745231628 s\n",
            "------------------------------\n",
            "[Epoch: 13 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3747 | train auc : 0.8588 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3369 | train auc : 0.8595 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3704 | train auc : 0.8227 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3524 | train auc : 0.8132 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3652 | train auc : 0.8046 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3704 | train auc : 0.8135 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3525 | train auc : 0.8236 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3608 | train auc : 0.821 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3707 | train auc : 0.8147 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3738 | train auc : 0.8094 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3724 | train auc : 0.8094 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.4677 | val auc : 1.0 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.4927 | val auc : 0.9657 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.537 | val auc : 0.9144 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5495 | val auc : 0.9199 | lr : 3e-05\n",
            "[Epoch: 13 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5854 | val auc : 0.8647 | lr : 3e-05\n",
            "train loss : 0.3728 | train auc 0.8071 | val loss 0.5746 | val auc 0.8824 | elapsed time 180.8304898738861 s\n",
            "------------------------------\n",
            "[Epoch: 14 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3238 | train auc : 0.8186 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3463 | train auc : 0.8425 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3259 | train auc : 0.8576 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3531 | train auc : 0.828 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3506 | train auc : 0.802 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3485 | train auc : 0.814 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3654 | train auc : 0.804 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3631 | train auc : 0.8063 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3668 | train auc : 0.8045 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3707 | train auc : 0.8072 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3697 | train auc : 0.8069 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5145 | val auc : 0.9184 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.544 | val auc : 0.9258 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5663 | val auc : 0.8836 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5748 | val auc : 0.8504 | lr : 3e-05\n",
            "[Epoch: 14 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5727 | val auc : 0.8434 | lr : 3e-05\n",
            "train loss : 0.3718 | train auc 0.8079 | val loss 0.5494 | val auc 0.8611 | elapsed time 179.87329244613647 s\n",
            "------------------------------\n",
            "[Epoch: 15 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.326 | train auc : 0.814 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3306 | train auc : 0.829 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3242 | train auc : 0.8357 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3453 | train auc : 0.811 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.341 | train auc : 0.8277 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3491 | train auc : 0.8325 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.351 | train auc : 0.835 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3509 | train auc : 0.8259 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3543 | train auc : 0.8225 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3539 | train auc : 0.8284 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.355 | train auc : 0.8297 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5247 | val auc : 0.9455 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5626 | val auc : 0.9167 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5922 | val auc : 0.82 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5706 | val auc : 0.844 | lr : 3e-05\n",
            "[Epoch: 15 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5688 | val auc : 0.8608 | lr : 3e-05\n",
            "train loss : 0.3559 | train auc 0.8265 | val loss 0.5631 | val auc 0.851 | elapsed time 178.52363753318787 s\n",
            "------------------------------\n",
            "[Epoch: 16 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.2659 | train auc : 0.8619 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.2694 | train auc : 0.884 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3056 | train auc : 0.8634 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3221 | train auc : 0.8572 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3378 | train auc : 0.8485 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3487 | train auc : 0.8254 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3449 | train auc : 0.8297 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3436 | train auc : 0.8315 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3529 | train auc : 0.8244 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3495 | train auc : 0.8259 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3551 | train auc : 0.8274 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.6574 | val auc : 0.8182 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.6558 | val auc : 0.7929 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.6306 | val auc : 0.8355 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5892 | val auc : 0.8679 | lr : 3e-05\n",
            "[Epoch: 16 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5845 | val auc : 0.8473 | lr : 3e-05\n",
            "train loss : 0.3565 | train auc 0.8273 | val loss 0.5797 | val auc 0.8566 | elapsed time 183.0282106399536 s\n",
            "------------------------------\n",
            "[Epoch: 17 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3306 | train auc : 0.878 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.2944 | train auc : 0.881 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3099 | train auc : 0.8573 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3057 | train auc : 0.8732 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3271 | train auc : 0.8416 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3395 | train auc : 0.8194 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3385 | train auc : 0.8211 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3496 | train auc : 0.8159 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3501 | train auc : 0.8218 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3499 | train auc : 0.8244 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3516 | train auc : 0.8194 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.4456 | val auc : 0.9815 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5266 | val auc : 0.9593 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5384 | val auc : 0.9062 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5368 | val auc : 0.8914 | lr : 3e-05\n",
            "[Epoch: 17 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5375 | val auc : 0.8653 | lr : 3e-05\n",
            "train loss : 0.3524 | train auc 0.8236 | val loss 0.5623 | val auc 0.8457 | elapsed time 182.7981276512146 s\n",
            "------------------------------\n",
            "[Epoch: 18 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.2739 | train auc : 0.8953 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.335 | train auc : 0.8448 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3284 | train auc : 0.8218 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3287 | train auc : 0.8314 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3327 | train auc : 0.8441 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3607 | train auc : 0.8256 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3573 | train auc : 0.8335 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.354 | train auc : 0.8365 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3445 | train auc : 0.8381 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.351 | train auc : 0.8323 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3553 | train auc : 0.8286 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5502 | val auc : 0.8173 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.6781 | val auc : 0.7297 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.617 | val auc : 0.8244 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.614 | val auc : 0.8512 | lr : 3e-05\n",
            "[Epoch: 18 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.602 | val auc : 0.8666 | lr : 3e-05\n",
            "train loss : 0.3532 | train auc 0.8302 | val loss 0.5839 | val auc 0.8591 | elapsed time 183.08305048942566 s\n",
            "------------------------------\n",
            "[Epoch: 19 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3254 | train auc : 0.8333 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3539 | train auc : 0.8144 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3414 | train auc : 0.8275 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3411 | train auc : 0.8345 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3573 | train auc : 0.8192 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3544 | train auc : 0.8184 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3602 | train auc : 0.8186 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3653 | train auc : 0.8214 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.368 | train auc : 0.8142 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3643 | train auc : 0.8115 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.357 | train auc : 0.8222 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5628 | val auc : 0.9167 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5339 | val auc : 0.8857 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5507 | val auc : 0.8621 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.544 | val auc : 0.8477 | lr : 3e-05\n",
            "[Epoch: 19 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5648 | val auc : 0.8227 | lr : 3e-05\n",
            "train loss : 0.3562 | train auc 0.8246 | val loss 0.5483 | val auc 0.8443 | elapsed time 188.387033700943 s\n",
            "------------------------------\n",
            "[Epoch: 20 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.2289 | train auc : 0.9551 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.2849 | train auc : 0.8586 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3055 | train auc : 0.8645 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3029 | train auc : 0.8541 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3038 | train auc : 0.8544 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.3236 | train auc : 0.8477 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.329 | train auc : 0.8442 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3342 | train auc : 0.838 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3362 | train auc : 0.8377 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3411 | train auc : 0.8381 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3372 | train auc : 0.8445 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.6416 | val auc : 0.8148 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.6335 | val auc : 0.7895 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.627 | val auc : 0.8136 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.568 | val auc : 0.8608 | lr : 3e-05\n",
            "[Epoch: 20 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5402 | val auc : 0.8842 | lr : 3e-05\n",
            "train loss : 0.341 | train auc 0.8419 | val loss 0.5551 | val auc 0.8547 | elapsed time 186.6534309387207 s\n",
            "------------------------------\n",
            "[Epoch: 21 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.3109 | train auc : 0.8264 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3634 | train auc : 0.8122 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3587 | train auc : 0.8174 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3654 | train auc : 0.8272 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3653 | train auc : 0.8229 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.349 | train auc : 0.8285 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3449 | train auc : 0.8395 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3467 | train auc : 0.8415 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.3541 | train auc : 0.8399 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3469 | train auc : 0.8439 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3368 | train auc : 0.8485 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.5353 | val auc : 0.8654 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.5581 | val auc : 0.8502 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.5391 | val auc : 0.8411 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5577 | val auc : 0.8329 | lr : 3e-05\n",
            "[Epoch: 21 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.5554 | val auc : 0.847 | lr : 3e-05\n",
            "train loss : 0.3399 | train auc 0.8463 | val loss 0.5529 | val auc 0.8457 | elapsed time 187.01211023330688 s\n",
            "------------------------------\n",
            "[Epoch: 22 / 50 |Single batch number : 100 / 1130 ]| avg train loss 0.4043 | train auc : 0.8196 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 200 / 1130 ]| avg train loss 0.3501 | train auc : 0.862 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 300 / 1130 ]| avg train loss 0.3319 | train auc : 0.8723 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 400 / 1130 ]| avg train loss 0.3226 | train auc : 0.8824 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 500 / 1130 ]| avg train loss 0.3293 | train auc : 0.8688 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 600 / 1130 ]| avg train loss 0.34 | train auc : 0.8594 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 700 / 1130 ]| avg train loss 0.3501 | train auc : 0.8499 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 800 / 1130 ]| avg train loss 0.3478 | train auc : 0.8491 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 900 / 1130 ]| avg train loss 0.342 | train auc : 0.8482 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 1000 / 1130 ]| avg train loss 0.3407 | train auc : 0.8501 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 1100 / 1130 ]| avg train loss 0.3397 | train auc : 0.8501 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 20 / 120 ] | avg val loss 0.4726 | val auc : 0.7375 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 40 / 120 ] | avg val loss 0.4895 | val auc : 0.9051 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 60 / 120 ] | avg val loss 0.4811 | val auc : 0.9048 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 80 / 120 ] | avg val loss 0.5542 | val auc : 0.8605 | lr : 3e-05\n",
            "[Epoch: 22 / 50 |Single batch number : 100 / 120 ] | avg val loss 0.568 | val auc : 0.8528 | lr : 3e-05\n",
            "train loss : 0.3372 | train auc 0.8517 | val loss 0.5903 | val auc 0.851 | elapsed time 186.10871696472168 s\n",
            "------------------------------\n",
            "Early stopping after 10 iterations without the decrease of the val loss\n",
            "training took 4698.607181310654 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}